data:
  batch_size: 32
  num_workers: 4
  persistent_workers: true
  pin_memory: true
  prefetch_factor: 2
  preprocessing:
    handle_categorical: one_hot
    handle_missing: mean
    normalize: true
hyperparameters:
  bias_init: 0.0
  dropout_rate: 0.0
  weight_initialization: xavier_uniform
logging:
  log_every_n_steps: 10
  metrics:
  - accuracy
  - f1
  - precision
  - recall
  save_dir: logs/logistic_regression
  save_visualization: true
  tensorboard: true
model:
  input_dim: 10
  output_dim: 1
  task: binary_classification
  type: logistic_regression
optimization:
  gradient_accumulation_steps: 1
  gradient_clip: true
  gradient_clip_val: 1.0
  mixed_precision: true
  optimizer:
    betas: !!python/tuple
    - 0.9
    - 0.999
    epsilon: 1.0e-08
    type: adam
    weight_decay: 0.01
  scheduler:
    min_lr: 1.0e-06
    type: cosine
    warmup_epochs: 3
training:
  batch_size: 32
  early_stopping: true
  epochs: 50
  gradient_clip_val: 1.0
  learning_rate: 0.001
  metrics:
  - accuracy
  - precision
  - recall
  - f1
  patience: 5
  validation_split: 0.2
  visualization: true
